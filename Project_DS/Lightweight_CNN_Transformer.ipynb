{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP83Sb5ma63hSVzfxw+moKT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer-Tahir/Project_DS/blob/main/Project_DS/Lightweight_CNN_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGzvuYehzPwW",
        "outputId": "8b3ff602-b345-4614-c3cb-ffb0c13c32ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Project_DS'...\n",
            "remote: Enumerating objects: 366, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 366 (delta 66), reused 9 (delta 9), pack-reused 268 (from 2)\u001b[K\n",
            "Receiving objects: 100% (366/366), 25.96 MiB | 12.92 MiB/s, done.\n",
            "Resolving deltas: 100% (138/138), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Sameer-Tahir/Project_DS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd   # data processing\n",
        "import numpy as np    # linear algebra\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_1L0CMFNzQ1M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Load your splits\n",
        "# -----------------------\n",
        "train_df = pd.read_csv(\"dfk1_train.csv\")\n",
        "val_df   = pd.read_csv(\"dfk1_val.csv\")\n",
        "test_df  = pd.read_csv(\"dfk1_test.csv\")"
      ],
      "metadata": {
        "id": "0-mf-UYdzQyc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o dl_preprocessed.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yby1KiwGzQv0",
        "outputId": "a7e3b674-b8e1-47dc-8cc2-83a973f0d16c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dl_preprocessed.zip\n",
            "  inflating: dl_data_windows.npz     \n",
            "  inflating: label_encoder.pkl       \n",
            "  inflating: scaler.pkl              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boruta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8BKq7T80r86",
        "outputId": "ed480f02-cf4b-4ca3-cdca-8705bf573015"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boruta\n",
            "  Downloading Boruta-0.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.12/dist-packages (from boruta) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from boruta) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from boruta) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->boruta) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->boruta) (3.6.0)\n",
            "Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: boruta\n",
            "Successfully installed boruta-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from boruta import BorutaPy"
      ],
      "metadata": {
        "id": "fln-q9AX0uwZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, joblib\n",
        "\n",
        "# -----------------------\n",
        "# Load preprocessed arrays\n",
        "# -----------------------\n",
        "data = np.load(\"dl_data_windows.npz\", allow_pickle=True)\n",
        "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
        "X_val, y_val     = data[\"X_val\"],   data[\"y_val\"]\n",
        "X_test, y_test   = data[\"X_test\"],  data[\"y_test\"]\n",
        "feature_cols     = data[\"feature_cols\"].tolist()\n",
        "classes          = data[\"classes\"]\n",
        "\n",
        "# Load encoder & scaler\n",
        "le = joblib.load(\"label_encoder.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "print(\"✅ Data loaded successfully\")\n",
        "print(\"Train:\", X_train.shape, y_train.shape)\n",
        "print(\"Val:\",   X_val.shape, y_val.shape)\n",
        "print(\"Test:\",  X_test.shape, y_test.shape)\n",
        "print(\"Classes:\", classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwNS_VjCzQtF",
        "outputId": "297fe1c8-50b8-488b-dd1c-6c0676abaf98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data loaded successfully\n",
            "Train: (4835, 6, 222) (4835,)\n",
            "Val: (591, 6, 222) (591,)\n",
            "Test: (590, 6, 222) (590,)\n",
            "Classes: ['aggressive-scan' 'cryptojacking' 'icmp-flood' 'icmp-fragmentation'\n",
            " 'icmp-fragmentation_old' 'none' 'os-fingerprinting' 'os-scan' 'port-scan'\n",
            " 'push-ack-flood' 'serice-detection' 'service-detection' 'syn-flood'\n",
            " 'syn-stealth' 'synonymous-ip-flood' 'tcp-flood' 'udp-flood' 'vuln-scan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Flatten windows: (n, window, n_features) -> (n, window*n_features)\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_val_flat   = X_val.reshape(X_val.shape[0],   -1)\n",
        "X_test_flat  = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print(\"Flattened shapes:\", X_train_flat.shape, X_val_flat.shape, X_test_flat.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# χ² selector (fast, formal)\n",
        "# ---------------------------\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "# chi2 expects non-negative; your values are in [0,1] after MinMaxScaler, so fine.\n",
        "k_keep = min(100, X_train_flat.shape[1])  # tune k if you want\n",
        "chi2_sel = SelectKBest(score_func=chi2, k=k_keep)\n",
        "X_train_chi2 = chi2_sel.fit_transform(X_train_flat, y_train)\n",
        "X_val_chi2   = chi2_sel.transform(X_val_flat)\n",
        "X_test_chi2  = chi2_sel.transform(X_test_flat)\n",
        "print(\"✅ χ² kept:\", X_train_chi2.shape[1])\n",
        "\n",
        "# ---------------------------\n",
        "# Boruta selector (wrapper)\n",
        "# ---------------------------\n",
        "from boruta import BorutaPy\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=500, max_depth=6, class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
        ")\n",
        "boruta = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=42)\n",
        "boruta.fit(X_train_flat, y_train)\n",
        "X_train_boruta = boruta.transform(X_train_flat)\n",
        "X_val_boruta   = boruta.transform(X_val_flat)\n",
        "X_test_boruta  = boruta.transform(X_test_flat)\n",
        "print(\"✅ Boruta kept:\", X_train_boruta.shape[1])\n",
        "\n",
        "# Keep masks (for optional DL channel pruning)\n",
        "chi2_mask   = np.zeros(X_train_flat.shape[1], dtype=bool)\n",
        "chi2_mask[chi2_sel.get_support(indices=True)] = True\n",
        "\n",
        "boruta_mask = np.zeros(X_train_flat.shape[1], dtype=bool)\n",
        "boruta_mask[np.where(boruta.support_)[0]] = True\n",
        "\n",
        "# Map flattened indices back to per-timestep feature indices\n",
        "window = X_train.shape[1]\n",
        "n_feat = X_train.shape[2]\n",
        "\n",
        "def flatmask_to_feature_idx(flat_mask, window, n_feat):\n",
        "    feat_idx = np.where(flat_mask)[0] % n_feat\n",
        "    return sorted(np.unique(feat_idx))\n",
        "\n",
        "chi2_feat_idx   = flatmask_to_feature_idx(chi2_mask,   window, n_feat)\n",
        "boruta_feat_idx = flatmask_to_feature_idx(boruta_mask, window, n_feat)\n",
        "\n",
        "print(f\"χ² feature columns kept (per-timestep): {len(chi2_feat_idx)}\")\n",
        "print(f\"Boruta feature columns kept (per-timestep): {len(boruta_feat_idx)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmphU57fzQp-",
        "outputId": "337f9943-8bd2-4df7-b18a-62b20857fefc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened shapes: (4835, 1332) (591, 1332) (590, 1332)\n",
            "✅ χ² kept: 100\n",
            "✅ Boruta kept: 390\n",
            "χ² feature columns kept (per-timestep): 27\n",
            "Boruta feature columns kept (per-timestep): 162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gESp8sQY84x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "Xtr_fs, Xva_fs, Xte_fs = X_train_boruta, X_val_boruta, X_test_boruta\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=600, learning_rate=0.05,\n",
        "    num_leaves=64, subsample=0.8, colsample_bytree=0.8,\n",
        "    max_depth=-1, objective=\"multiclass\",\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42\n",
        ")\n",
        "lgb_model.fit(Xtr_fs, y_train, eval_set=[(Xva_fs, y_val)], eval_metric=\"multi_logloss\", verbose=False)\n",
        "\n",
        "y_pred_ml   = lgb_model.predict(Xte_fs)\n",
        "y_proba_ml  = lgb_model.predict_proba(Xte_fs)\n",
        "\n",
        "acc_ml = accuracy_score(y_test, y_pred_ml)\n",
        "f1_ml  = f1_score(y_test, y_pred_ml, average=\"macro\")\n",
        "print(f\"🔵 LightGBM+FS  Acc={acc_ml:.4f}  MacroF1={f1_ml:.4f}\")\n",
        "\n",
        "report_ml = classification_report(y_test, y_pred_ml, target_names=classes, zero_division=0)\n",
        "cm_ml     = confusion_matrix(y_test, y_pred_ml)\n",
        "\n",
        "# Save ML results\n",
        "with open(\"results_ml_lgb_boruta.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"accuracy\": float(acc_ml),\n",
        "        \"macro_f1\": float(f1_ml),\n",
        "        \"report\": report_ml,\n",
        "        \"confusion_matrix\": cm_ml.tolist()\n",
        "    }, f, indent=2)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "nRpkCFbMzcQj",
        "outputId": "4537296c-95a3-4767-e395-5953ffc9d7e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LGBMClassifier.fit() got an unexpected keyword argument 'verbose'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2602718969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva_fs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi_logloss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my_pred_ml\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "num_classes = len(classes)\n",
        "\n",
        "# ---------- OPTIONAL: reduce feature channels for DL too (using Boruta columns)\n",
        "use_feature_pruning = False  # set True to prune DL channels\n",
        "if use_feature_pruning:\n",
        "    keep_cols = boruta_feat_idx  # or chi2_feat_idx\n",
        "    X_train_dl = X_train[:, :, keep_cols]\n",
        "    X_val_dl   = X_val[:,   :, keep_cols]\n",
        "    X_test_dl  = X_test[:,  :, keep_cols]\n",
        "else:\n",
        "    X_train_dl, X_val_dl, X_test_dl = X_train, X_val, X_test\n",
        "\n",
        "window  = X_train_dl.shape[1]\n",
        "n_feat  = X_train_dl.shape[2]\n",
        "\n",
        "# ---------- Multiclass focal loss (correct version)\n",
        "def categorical_focal_loss(gamma=2.0, alpha=None):\n",
        "    \"\"\"\n",
        "    y_true: int labels (will be one-hot)\n",
        "    y_pred: softmax probabilities\n",
        "    alpha:  class weights array-like of shape [num_classes] or None\n",
        "    \"\"\"\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true_oh = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-8, 1.0)\n",
        "        ce = -tf.reduce_sum(y_true_oh * tf.math.log(y_pred), axis=-1)  # CE per sample\n",
        "        p_t = tf.reduce_sum(y_true_oh * y_pred, axis=-1)               # prob of true class\n",
        "        modulating = tf.pow(1.0 - p_t, gamma)\n",
        "        if alpha is not None:\n",
        "            alpha_t = tf.reduce_sum(y_true_oh * tf.constant(alpha, dtype=tf.float32), axis=-1)\n",
        "            fl = alpha_t * modulating * ce\n",
        "        else:\n",
        "            fl = modulating * ce\n",
        "        return tf.reduce_mean(fl)\n",
        "    return loss\n",
        "\n",
        "# ---------- Class weights -> alpha for focal loss (normalize)\n",
        "cls_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "alpha = (cls_weights / np.sum(cls_weights)).astype(np.float32)  # normalize for stability\n",
        "print(\"DL alpha (from class weights):\", alpha)\n",
        "\n",
        "# ---------- Lightweight CNN + Transformer head\n",
        "def build_cnn_tr(window, n_features, n_classes, emb=64, heads=4, ff=128, blocks=2, drop=0.3):\n",
        "    inp = layers.Input(shape=(window, n_features))\n",
        "\n",
        "    # small CNN stem\n",
        "    x = layers.Conv1D(96, 3, padding='same', activation='relu')(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv1D(96, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling1D(2)(x)  # (window/2, 96)\n",
        "\n",
        "    # projection to transformer dims\n",
        "    x = layers.Dense(emb)(x)\n",
        "\n",
        "    # a couple of transformer encoder blocks\n",
        "    for _ in range(blocks):\n",
        "        attn = layers.MultiHeadAttention(num_heads=heads, key_dim=emb)(x, x)\n",
        "        x = layers.Add()([x, attn])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "        ffw = layers.Dense(ff, activation='relu')(x)\n",
        "        ffw = layers.Dense(emb)(ffw)\n",
        "        x = layers.Add()([x, ffw])\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(drop)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=categorical_focal_loss(gamma=2.0, alpha=alpha),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "dl_model = build_cnn_tr(window, n_feat, num_classes)\n",
        "\n",
        "early = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n",
        "rlr   = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "ckpt  = callbacks.ModelCheckpoint(\"DL_final.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "\n",
        "hist = dl_model.fit(\n",
        "    X_train_dl, y_train,\n",
        "    validation_data=(X_val_dl, y_val),\n",
        "    epochs=60, batch_size=32,\n",
        "    callbacks=[early, rlr, ckpt],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_proba_dl = dl_model.predict(X_test_dl)\n",
        "y_pred_dl  = y_proba_dl.argmax(axis=1)\n",
        "acc_dl = accuracy_score(y_test, y_pred_dl)\n",
        "f1_dl  = f1_score(y_test, y_pred_dl, average=\"macro\")\n",
        "print(f\"🟣 DL (CNN+Transformer+Focal)  Acc={acc_dl:.4f}  MacroF1={f1_dl:.4f}\")\n",
        ""
      ],
      "metadata": {
        "id": "VMfd_HX3zcN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure class order is the same; both use the same label encoder, so OK.\n",
        "from scipy.special import softmax\n",
        "# y_proba_ml: shape (n_samples, num_classes) from LightGBM\n",
        "# y_proba_dl: already computed above from DL\n",
        "\n",
        "# Simple average (you can weight, e.g., 0.6*ML + 0.4*DL)\n",
        "w_ml, w_dl = 0.5, 0.5\n",
        "proba_ens = w_ml * y_proba_ml + w_dl * y_proba_dl\n",
        "y_pred_ens = proba_ens.argmax(axis=1)\n",
        "\n",
        "acc_ens = accuracy_score(y_test, y_pred_ens)\n",
        "f1_ens  = f1_score(y_test, y_pred_ens, average=\"macro\")\n",
        "print(f\"🟢 Ensemble  Acc={acc_ens:.4f}  MacroF1={f1_ens:.4f}\")\n"
      ],
      "metadata": {
        "id": "siZ9-nemzcLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def plot_cm(cm, labels, title, filename):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"📊 Saved {filename}\")\n",
        "\n",
        "def plot_training_curves(history, title_prefix=\"DL\"):\n",
        "    h = history.history\n",
        "    # Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(h[\"accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(h[\"val_accuracy\"], label=\"val_acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(f\"{title_prefix} Accuracy\")\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(f\"{title_prefix}_accuracy.png\", dpi=200); plt.close()\n",
        "    # Loss\n",
        "    plt.figure()\n",
        "    plt.plot(h[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(h[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{title_prefix} Loss\")\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(f\"{title_prefix}_loss.png\", dpi=200); plt.close()\n",
        "    print(f\"📈 Saved {title_prefix}_accuracy.png and {title_prefix}_loss.png\")\n",
        "\n",
        "def per_class_table(y_true, y_pred, label_names):\n",
        "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred, labels=range(len(label_names)))\n",
        "    df = pd.DataFrame({\n",
        "        \"class\": label_names,\n",
        "        \"precision\": p, \"recall\": r, \"f1\": f1, \"support\": s\n",
        "    })\n",
        "    return df\n",
        "\n",
        "# --- Confusion matrices\n",
        "cm_ml  = confusion_matrix(y_test, y_pred_ml)\n",
        "cm_dl  = confusion_matrix(y_test, y_pred_dl)\n",
        "cm_ens = confusion_matrix(y_test, y_pred_ens)\n",
        "plot_cm(cm_ml,  classes, \"LightGBM+FS Confusion Matrix\", \"cm_lgb.png\")\n",
        "plot_cm(cm_dl,  classes, \"DL (Focal) Confusion Matrix\",   \"cm_dl.png\")\n",
        "plot_cm(cm_ens, classes, \"Ensemble Confusion Matrix\",     \"cm_ens.png\")\n",
        "\n",
        "# --- ROC (one-vs-rest)\n",
        "def plot_multiclass_roc(y_true, proba, labels, filename):\n",
        "    y_true_bin = label_binarize(y_true, classes=range(len(labels)))\n",
        "    plt.figure(figsize=(7,6))\n",
        "    for i, name in enumerate(labels):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], proba[:, i])\n",
        "        auc = roc_auc_score(y_true_bin[:, i], proba[:, i])\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.2f})\")\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (One-vs-Rest)\")\n",
        "    plt.legend(fontsize=8, ncol=2)\n",
        "    plt.tight_layout(); plt.savefig(filename, dpi=200); plt.close()\n",
        "    print(f\"🧭 Saved {filename}\")\n",
        "\n",
        "plot_multiclass_roc(y_test, y_proba_ml,  classes, \"roc_lgb.png\")\n",
        "plot_multiclass_roc(y_test, y_proba_dl,  classes, \"roc_dl.png\")\n",
        "plot_multiclass_roc(y_test, proba_ens,   classes, \"roc_ens.png\")\n",
        "\n",
        "# --- Training curves for DL\n",
        "plot_training_curves(hist, title_prefix=\"DL_final\")\n",
        "\n",
        "# --- Per-class tables & save\n",
        "df_ml  = per_class_table(y_test, y_pred_ml,  list(classes))\n",
        "df_dl  = per_class_table(y_test, y_pred_dl,  list(classes))\n",
        "df_ens = per_class_table(y_test, y_pred_ens, list(classes))\n",
        "\n",
        "df_ml.to_csv(\"per_class_ml.csv\",  index=False)\n",
        "df_dl.to_csv(\"per_class_dl.csv\",  index=False)\n",
        "df_ens.to_csv(\"per_class_ens.csv\",index=False)\n",
        "print(\"🗂️ Saved per-class CSVs: per_class_ml.csv, per_class_dl.csv, per_class_ens.csv\")\n"
      ],
      "metadata": {
        "id": "9wkaOyTLzcIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML\n",
        "import joblib\n",
        "joblib.dump(lgb_model, \"LGBM_FS_boruta.pkl\")\n",
        "\n",
        "with open(\"results_lgb_boruta.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"accuracy\": float(acc_ml),\n",
        "        \"macro_f1\": float(f1_ml),\n",
        "        \"report\": classification_report(y_test, y_pred_ml, target_names=classes, zero_division=0),\n",
        "        \"confusion_matrix\": cm_ml.tolist()\n",
        "    }, f, indent=2)\n",
        "\n",
        "# DL already saved best weights as \"DL_final.keras\" via ModelCheckpoint\n",
        "with open(\"results_dl_focal.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"accuracy\": float(acc_dl),\n",
        "        \"macro_f1\": float(f1_dl),\n",
        "        \"report\": classification_report(y_test, y_pred_dl, target_names=classes, zero_division=0),\n",
        "        \"confusion_matrix\": cm_dl.tolist()\n",
        "    }, f, indent=2)\n",
        "\n",
        "# Ensemble (no model to save, but save results)\n",
        "with open(\"results_ensemble.json\",\"w\") as f:\n",
        "    json.dump({\n",
        "        \"accuracy\": float(acc_ens),\n",
        "        \"macro_f1\": float(f1_ens),\n",
        "        \"report\": classification_report(y_test, y_pred_ens, target_names=classes, zero_division=0),\n",
        "        \"confusion_matrix\": cm_ens.tolist()\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"📦 Saved: LGBM_FS_boruta.pkl, DL_final.keras, results_*.json, cm_*.png, roc_*.png, DL_final_*curves.png\")\n"
      ],
      "metadata": {
        "id": "HljHaYPpzxPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzVSX8BSzxNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_OTawkPzxKr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}